{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from numba import njit\n",
    "import pandas as pd\n",
    "# from project import Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataSet = pd.read_csv('card_transdata.csv')\n",
    "dataSet = pd.read_csv('Book1.csv', header=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataSet.dropna(inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Handle outliers\n",
    "def handle_outliers(df):\n",
    "    # Select numerical columns only\n",
    "    num_cols = df.select_dtypes(include=[np.number])\n",
    "    \n",
    "    # Compute the 1st and 99th percentile of each numerical column\n",
    "    percentiles = np.nanpercentile(num_cols, [1, 99], axis=0)\n",
    "\n",
    "    # Winsorize the numerical columns\n",
    "    num_cols = np.clip(num_cols, percentiles[0], percentiles[1])\n",
    "\n",
    "    # Replace the original numerical columns in the dataframe with the winsorized ones\n",
    "    df[num_cols.columns] = num_cols\n",
    "\n",
    "# handle_outliers(dataSet)\n",
    "\n",
    "# for col in NUM_COL:\n",
    "#     plotting(dataSet, col)\n",
    "\n",
    "# dataSet.describe()\n",
    "(dataSet.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##convert the distance from home where >40 is Far and 15<distance<40 is Medium and <15 is Close\n",
    "#use function cut\n",
    "\n",
    "# dataSet['distance_from_home'] = pd.qcut(dataSet['distance_from_home'], q=3, labels=['Close_from_home', 'Medium_from_home', 'Far_from_home'])\n",
    "# dataSet['distance_from_last_transaction'] = pd.qcut(dataSet['distance_from_last_transaction'], q=3, labels=['Close_from_lt', 'Medium_from_lt', 'Far_from_lt'])\n",
    "# dataSet['ratio_to_median_purchase_price'] = pd.qcut(dataSet['ratio_to_median_purchase_price'], q=4, labels=['Low_ratio', 'Medium_ratio', 'High_ratio','Extreme_ratio'])\n",
    "# dataSet['repeat_retailer'] = pd.cut(dataSet['repeat_retailer'], bins=[-0.5, 0.9, np.inf], labels=['no_repeat', 'repeat'])\n",
    "# dataSet['used_chip'] = pd.cut(dataSet['used_chip'], bins=[-0.5, 0.9, np.inf], labels=['no_chip', 'chip'])\n",
    "# dataSet['used_pin_number'] = pd.cut(dataSet['used_pin_number'], bins=[-0.5, 0.9, np.inf], labels=['no_pin', 'pin'])\n",
    "# dataSet['online_order'] = pd.cut(dataSet['online_order'], bins=[-0.5, 0.9, np.inf], labels=['offline', 'online'])\n",
    "# dataSet['fraud'] = pd.cut(dataSet['fraud'], bins=[-0.5, 0.9, np.inf], labels=['not_fraud', 'fraud'])\n",
    "# (dataSet.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dataframe into a list of transactions\n",
    "transactions = []\n",
    "for i in tqdm(range(len(dataSet))):\n",
    "    transactions.append([str(item) for item in dataSet.iloc[i]])\n",
    "    \n",
    "print(transactions[0:5])\n",
    "\n",
    "# Set the minimum support and confidence thresholds\n",
    "min_support = 2/9\n",
    "min_confidence = 0.5\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a list of frequent 1-itemsets\n",
    "def generate_frequent_1_itemsets(transactions, min_support):\n",
    "    #key = item, value = count\n",
    "    item_counts = {}\n",
    "    frequent_items = []\n",
    "    for transaction in transactions:\n",
    "        for item in transaction:\n",
    "            if item in item_counts:\n",
    "                item_counts[item] += 1\n",
    "            else:\n",
    "                item_counts[item] = 1\n",
    "    \n",
    "    for item, count in item_counts.items():\n",
    "        if item == 'nan':\n",
    "            continue\n",
    "        support = count / len(transactions)\n",
    "        if support >= min_support:\n",
    "            frequent_items.append(set(item))\n",
    "    return frequent_items\n",
    "frequent_itemsets = generate_frequent_1_itemsets(transactions, min_support)\n",
    "\n",
    "print((frequent_itemsets))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate frequent k-itemsets\n",
    "from itertools import combinations\n",
    "k = 2\n",
    "\n",
    "while len(frequent_itemsets) > 0:\n",
    "    candidate_itemsets :list[list] = []\n",
    "    # Generate candidate itemsets of size k\n",
    "    for i in range(len(frequent_itemsets)):\n",
    "        for j in range(i+1, len(frequent_itemsets)):\n",
    "            itemset1 = set(frequent_itemsets[i])\n",
    "            itemset2 = set(frequent_itemsets[j])\n",
    "            # print(\"itemset1\", itemset1, \"itemset2\", itemset2)\n",
    "            candidate_itemset = list(itemset1.union(itemset2))\n",
    "            # Prune the candidate itemsets\n",
    "            if all([set(itemset) in frequent_itemsets for itemset in combinations(candidate_itemset, k-1)]):\n",
    "                candidate_itemsets.append(candidate_itemset)\n",
    "            # print(frequent_itemsets)\n",
    "\n",
    "                # if itemset in frequent_itemsets:\n",
    "    \n",
    "                    \n",
    "    # Count the support of each candidate itemset\n",
    "    item_counts = {}\n",
    "    for transaction in transactions:\n",
    "        for candidate_itemset in candidate_itemsets:\n",
    "            if set(candidate_itemset).issubset(set(transaction)):\n",
    "                item = sorted(candidate_itemset)\n",
    "                if str(item) in item_counts:\n",
    "                    item_counts[str(item)] += 1\n",
    "                else:\n",
    "                    item_counts[str(item)] = 1\n",
    "\n",
    "    # print(\"item_counts\", item_counts)\n",
    "    # Generate a list of frequent k-itemsets\n",
    "    frequent_itemsets = []\n",
    "    for itemset, count in item_counts.items():\n",
    "        support = count / len(transactions)\n",
    "        if support >= min_support:\n",
    "            # print(\"itemset\", itemset)\n",
    "            tempSet = set()\n",
    "            for item in eval(itemset):\n",
    "                tempSet.add(item)\n",
    "            frequent_itemsets.append(tempSet)\n",
    "    \n",
    "    print(\"frequent_itemsets\", frequent_itemsets)\n",
    "    k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(frequent_itemsets)\n",
    "print(candidate_itemsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
