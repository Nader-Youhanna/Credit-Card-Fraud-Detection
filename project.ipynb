{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numba import jit, cuda\n",
    "\n",
    "\n",
    "CAT_COL = [\"repeat_retailer\", \"used_chip\", \"used_pin_number\", \"online_order\", \"fraud\"]\n",
    "NUM_COL = [\"distance_from_home\", \"distance_from_last_transaction\", \"ratio_to_median_purchase_price\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting(df : pd.DataFrame, col : str):\n",
    "    df.plot.scatter(x=col, y=\"fraud\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance_from_home [57.87785658 10.8299427   5.09107949 ...  2.91485699  4.25872939\n",
      " 58.10812496]\n",
      "distance_from_last_transaction [0.31114001 0.1755915  0.80515259 ... 1.47268669 0.24202337 0.31811012]\n",
      "ratio_to_median_purchase_price [1.94593998 1.29421881 0.42771456 ... 0.21807549 0.47582206 0.38691985]\n",
      "repeat_retailer [1. 0.]\n",
      "used_chip [1. 0.]\n",
      "used_pin_number [0. 1.]\n",
      "online_order [0. 1.]\n",
      "fraud [0. 1.]\n"
     ]
    }
   ],
   "source": [
    "dataSet = pd.read_csv('card_transdata.csv')\n",
    "#print levels of every column\n",
    "for col in dataSet.columns:\n",
    "    print(col, dataSet[col].unique())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes:\n",
    "\n",
    "## 1. Payment Methods and Transaction Data\n",
    "\n",
    "When analyzing transaction data, it's important to consider the different payment methods that may have been used. The `used_chip`, `used_pin_number`, and `online_order` variables can provide valuable insights into the payment methods used for each transaction.\n",
    "\n",
    "Here are some possible interpretations of different combinations of these variables:\n",
    "\n",
    "- A row with `used_chip = 0`, `used_pin_number = 0`, and `online_order = 0` could indicate that the payment was made using a wireless payment method, such as a mobile payment app that uses Bluetooth or NFC technology to make payments.\n",
    "\n",
    "- A row with `used_chip = 1`, `used_pin_number = 0`, and `online_order = 1` could indicate that the payment was made using NFC technology, which allows users to make contactless payments using their smartphones or other devices.\n",
    "\n",
    "Other combinations of these variables could also indicate the use of alternative payment methods, such as bank transfers, prepaid cards, or cryptocurrency.\n",
    "\n",
    "When cleaning and preprocessing the data, it's important to carefully consider the implications of missing or incomplete data and to handle outliers appropriately. Additionally, z-score normalization can be used to standardize the scale of variables in the dataset, and feature engineering can be used to create new variables that may improve the predictive power of the model.\n",
    "\n",
    "Overall, understanding the different payment methods used in the transaction data can help to uncover valuable insights and improve the accuracy and effectiveness of the analysis.\n",
    "\n",
    "## 2. Outliers:\n",
    "\n",
    "The `distance_from_home`, `distance_from_last_transaction`, and `ratio_to_median_purchase_price` columns may contain outliers that are valid and meaningful data points. For example, a transaction that is much farther from home or the last transaction may indicate a change in the consumer's behavior or circumstances. Similarly, a purchase that is significantly above or below the median price may reflect the consumer's preferences or availability of products.\n",
    "Therefore, removing all outliers in these columns may result in a biased or incomplete representation of your data and lead to erroneous conclusions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSet.dropna(inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>distance_from_home</th>\n",
       "      <th>distance_from_last_transaction</th>\n",
       "      <th>ratio_to_median_purchase_price</th>\n",
       "      <th>repeat_retailer</th>\n",
       "      <th>used_chip</th>\n",
       "      <th>used_pin_number</th>\n",
       "      <th>online_order</th>\n",
       "      <th>fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.000999</td>\n",
       "      <td>-0.001911</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.881536</td>\n",
       "      <td>0.350399</td>\n",
       "      <td>0.100608</td>\n",
       "      <td>0.650552</td>\n",
       "      <td>0.087403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.994184</td>\n",
       "      <td>0.976605</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.323157</td>\n",
       "      <td>0.477095</td>\n",
       "      <td>0.300809</td>\n",
       "      <td>0.476796</td>\n",
       "      <td>0.282425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.828117</td>\n",
       "      <td>-1.273700</td>\n",
       "      <td>-0.589242</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.608439</td>\n",
       "      <td>-0.351449</td>\n",
       "      <td>-0.504026</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.341898</td>\n",
       "      <td>-0.252275</td>\n",
       "      <td>-0.355299</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.199814</td>\n",
       "      <td>-0.037294</td>\n",
       "      <td>0.030011</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.883770</td>\n",
       "      <td>5.992616</td>\n",
       "      <td>5.749955</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       distance_from_home  distance_from_last_transaction  \\\n",
       "count      1000000.000000                  1000000.000000   \n",
       "mean            -0.000999                       -0.001911   \n",
       "std              0.994184                        0.976605   \n",
       "min             -0.828117                       -1.273700   \n",
       "25%             -0.608439                       -0.351449   \n",
       "50%             -0.341898                       -0.252275   \n",
       "75%              0.199814                       -0.037294   \n",
       "max              4.883770                        5.992616   \n",
       "\n",
       "       ratio_to_median_purchase_price  repeat_retailer       used_chip  \\\n",
       "count                  1000000.000000   1000000.000000  1000000.000000   \n",
       "mean                         0.000001         0.881536        0.350399   \n",
       "std                          0.999997         0.323157        0.477095   \n",
       "min                         -0.589242         0.000000        0.000000   \n",
       "25%                         -0.504026         1.000000        0.000000   \n",
       "50%                         -0.355299         1.000000        0.000000   \n",
       "75%                          0.030011         1.000000        1.000000   \n",
       "max                          5.749955         1.000000        1.000000   \n",
       "\n",
       "       used_pin_number    online_order           fraud  \n",
       "count   1000000.000000  1000000.000000  1000000.000000  \n",
       "mean          0.100608        0.650552        0.087403  \n",
       "std           0.300809        0.476796        0.282425  \n",
       "min           0.000000        0.000000        0.000000  \n",
       "25%           0.000000        0.000000        0.000000  \n",
       "50%           0.000000        1.000000        0.000000  \n",
       "75%           0.000000        1.000000        0.000000  \n",
       "max           1.000000        1.000000        1.000000  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Handle outliers\n",
    "def handle_outliers(df):\n",
    "    # Select numerical columns only\n",
    "    num_cols = df.select_dtypes(include=[np.number])\n",
    "    \n",
    "    # Compute the 1st and 99th percentile of each numerical column\n",
    "    percentiles = np.nanpercentile(num_cols, [1, 99], axis=0)\n",
    "\n",
    "    # Winsorize the numerical columns\n",
    "    num_cols = np.clip(num_cols, percentiles[0], percentiles[1])\n",
    "\n",
    "    # Replace the original numerical columns in the dataframe with the winsorized ones\n",
    "    df[num_cols.columns] = num_cols\n",
    "\n",
    "handle_outliers(dataSet)\n",
    "\n",
    "# for col in NUM_COL:\n",
    "#     plotting(dataSet, col)\n",
    "\n",
    "dataSet.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "### PCA: Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(X):\n",
    "    # Center the data\n",
    "    Y = X - np.mean(X, axis=0)\n",
    "\n",
    "    # Compute the covariance matrix\n",
    "    cov = np.cov(Y.T)\n",
    "    \n",
    "    # Compute Eigenvectors matrix\n",
    "    eigenvectors, _, _ = np.linalg.svd(cov)\n",
    "\n",
    "    # Project data onto new subspace\n",
    "    Z = np.matmul(Y, eigenvectors)\n",
    "\n",
    "    return Z\n",
    "\n",
    "dataSet[NUM_COL] = pd.DataFrame(pca(np.array(dataSet[NUM_COL])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize the data\n",
    "Note how now the numerical columns all have mean $\\approx$ 0 and std = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>distance_from_home</th>\n",
       "      <th>distance_from_last_transaction</th>\n",
       "      <th>ratio_to_median_purchase_price</th>\n",
       "      <th>repeat_retailer</th>\n",
       "      <th>used_chip</th>\n",
       "      <th>used_pin_number</th>\n",
       "      <th>online_order</th>\n",
       "      <th>fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.000000e+06</td>\n",
       "      <td>1.000000e+06</td>\n",
       "      <td>1.000000e+06</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-9.201528e-19</td>\n",
       "      <td>6.376233e-18</td>\n",
       "      <td>4.915179e-18</td>\n",
       "      <td>0.881536</td>\n",
       "      <td>0.350399</td>\n",
       "      <td>0.100608</td>\n",
       "      <td>0.650552</td>\n",
       "      <td>0.087403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.323157</td>\n",
       "      <td>0.477095</td>\n",
       "      <td>0.300809</td>\n",
       "      <td>0.476796</td>\n",
       "      <td>0.282425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-8.319567e-01</td>\n",
       "      <td>-1.302255e+00</td>\n",
       "      <td>-5.892455e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-6.109938e-01</td>\n",
       "      <td>-3.579117e-01</td>\n",
       "      <td>-5.040292e-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-3.428929e-01</td>\n",
       "      <td>-2.563624e-01</td>\n",
       "      <td>-3.553012e-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.019876e-01</td>\n",
       "      <td>-3.623103e-02</td>\n",
       "      <td>3.000966e-02</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.913347e+00</td>\n",
       "      <td>6.138127e+00</td>\n",
       "      <td>5.749972e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       distance_from_home  distance_from_last_transaction  \\\n",
       "count        1.000000e+06                    1.000000e+06   \n",
       "mean        -9.201528e-19                    6.376233e-18   \n",
       "std          1.000000e+00                    1.000000e+00   \n",
       "min         -8.319567e-01                   -1.302255e+00   \n",
       "25%         -6.109938e-01                   -3.579117e-01   \n",
       "50%         -3.428929e-01                   -2.563624e-01   \n",
       "75%          2.019876e-01                   -3.623103e-02   \n",
       "max          4.913347e+00                    6.138127e+00   \n",
       "\n",
       "       ratio_to_median_purchase_price  repeat_retailer       used_chip  \\\n",
       "count                    1.000000e+06   1000000.000000  1000000.000000   \n",
       "mean                     4.915179e-18         0.881536        0.350399   \n",
       "std                      1.000000e+00         0.323157        0.477095   \n",
       "min                     -5.892455e-01         0.000000        0.000000   \n",
       "25%                     -5.040292e-01         1.000000        0.000000   \n",
       "50%                     -3.553012e-01         1.000000        0.000000   \n",
       "75%                      3.000966e-02         1.000000        1.000000   \n",
       "max                      5.749972e+00         1.000000        1.000000   \n",
       "\n",
       "       used_pin_number    online_order           fraud  \n",
       "count   1000000.000000  1000000.000000  1000000.000000  \n",
       "mean          0.100608        0.650552        0.087403  \n",
       "std           0.300809        0.476796        0.282425  \n",
       "min           0.000000        0.000000        0.000000  \n",
       "25%           0.000000        0.000000        0.000000  \n",
       "50%           0.000000        1.000000        0.000000  \n",
       "75%           0.000000        1.000000        0.000000  \n",
       "max           1.000000        1.000000        1.000000  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Standardization using Z-score normalization\n",
    "def standardize(df):\n",
    "    result = df.copy()\n",
    "    for feature_name in df.columns:\n",
    "        if feature_name in CAT_COL:\n",
    "            continue\n",
    "        mean_value = df[feature_name].mean()\n",
    "        std_value = df[feature_name].std()\n",
    "        result[feature_name] = (df[feature_name] - mean_value) / std_value\n",
    "    return result\n",
    "\n",
    "dataSet = standardize(dataSet)\n",
    "\n",
    "# Describe DataSet after Standardization\n",
    "dataSet.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data\n",
    "- Test Dataset: 20%\n",
    "- Train Dataset: 80% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "x_train, x_test, y_train, y_test = train_test_split(dataSet.drop('fraud',axis=1), dataSet['fraud'],\n",
    "                                                     test_size=0.2, random_state=42)\n",
    "# Convert the data into numpy arrays\n",
    "x_train = x_train.values\n",
    "x_test = x_test.values\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of features\n",
    "N = x_train.shape[1]\n",
    "print(f'Number of features = {N}')\n",
    "\n",
    "#Number of training examples\n",
    "M = x_train.shape[0]\n",
    "print(f'Number of training examples = {M}')\n",
    "\n",
    "#Number of testing examples\n",
    "M_test = x_test.shape[0]\n",
    "print(f'Number of testing examples = {M_test}')\n",
    "\n",
    "#Number of classes\n",
    "K = len(np.unique(y_train))\n",
    "print(f'Number of classes = {K}')\n",
    "\n",
    "# Print shapes of training and testing data\n",
    "print(f'Shape of x_train = {x_train.shape} = (M, N)')\n",
    "print(f'Shape of y_train = {y_train.shape} = (M, )')\n",
    "print(f'Shape of x_test = {x_test.shape} = (M_test, N)')\n",
    "print(f'Shape of y_test = {y_test.shape} = (M_test, )')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement logistic regression\n",
    "logisticRegr = LogisticRegression(max_iter=1000)\n",
    "logisticRegr.fit(x_train, y_train)\n",
    "# predictions = logisticRegr.predict(x_test)\n",
    "# print(\"Logistic regression predictions: \", predictions[:10])\n",
    "score = logisticRegr.score(x_test, y_test)\n",
    "print(f\"Logisitic regression score: {round(score*100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get user input for one transaction\n",
    "def predict_single(model ,distance_from_home, distance_from_last_transaction, \n",
    "                       ratio_to_median_purchase_price, repeat_retailer, used_chip, \n",
    "                       used_pin_number, online_order):\n",
    "    # distance_from_home = 270\n",
    "    # distance_from_last_transaction = 500\n",
    "    # ratio_to_median_purchase_price = 1.2\n",
    "    # repeat_retailer = 0\n",
    "    # used_chip = 1\n",
    "    # used_pin_number = 0\n",
    "    # online_order = 0\n",
    "\n",
    "    # row ha shape (1, N)\n",
    "    row = np.array([[ distance_from_home,\n",
    "                    distance_from_last_transaction,\n",
    "                    ratio_to_median_purchase_price,\n",
    "                    repeat_retailer,\n",
    "                    used_chip, used_pin_number, online_order]])\n",
    "    \n",
    "    prediction = int(model.predict(row))\n",
    "    return prediction\n",
    "\n",
    "print(f'Prediction: {predict_single(logisticRegr, 5, 1, 0.8, 0, 1, 1, 0)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimum Distance Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateDistance(x1, x2):\n",
    "    # Euclidean distance\n",
    "    distance = np.linalg.norm(x1-x2)\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MinimumDistanceClassifier(test_points, training_features, labels):\n",
    "    # INPUTS:   test_points: (M_test, N)  \n",
    "    #           training_features: (M, N)\n",
    "    #           labels: (M, )\n",
    "    \n",
    "    # OUTPUTS:  classification: an integer indicating the classification of the test point\n",
    "    #                           either 0 (No Fraud) or 1 (Fraud)\n",
    "      \n",
    "    \n",
    "    zeros_mean = np.mean(training_features[labels == 0], axis = 0)\n",
    "    ones_mean = np.mean(training_features[labels == 1], axis = 0)\n",
    "\n",
    "    zeros_dist = calculateDistance(zeros_mean, test_points)\n",
    "    ones_dist = calculateDistance(ones_mean, test_points)\n",
    "\n",
    "    if zeros_dist < ones_dist:\n",
    "        classification = 0\n",
    "    else:\n",
    "        classification = 1\n",
    "    \n",
    "    return classification\n",
    "\n",
    "mdc_accuracy = 0\n",
    "\n",
    "\n",
    "predictions = MinimumDistanceClassifier(x_test, x_train, y_train)\n",
    "mdc_accuracy = np.mean(predictions == y_test)\n",
    "print(f'Minimum Distance Classifier accuracy: {round(mdc_accuracy*100, 2)}%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function optimized to run on gpu \n",
    "@jit\n",
    "def KNN(test_points, training_features, labels, k):\n",
    "    # INPUTS:   test_points: (M_test, N)\n",
    "    #           training_features: (M, N)\n",
    "    #           k: the number of nearest neighbours. \n",
    "    \n",
    "    # OUTPUTS:  classification: an integer indicating the classification of the test point\n",
    "    #                           either 0 (No Fraud) or 1 (Fraud)\n",
    "\n",
    "    distances = [calculateDistance(test_points, p) for p in training_features]\n",
    "\n",
    "    # distances = np.sqrt(((test_points[:, np.newaxis, :] - training_features) ** 2).sum(axis=2))\n",
    "    \n",
    "    k_nearest = np.argpartition(distances, k)[:k]\n",
    "    \n",
    "    zeros_votes = 0\n",
    "    ones_votes = 0\n",
    "\n",
    "    for i in k_nearest:\n",
    "        if (labels[i] == 0):\n",
    "            zeros_votes += 1\n",
    "        if (labels[i] == 1):\n",
    "            ones_votes += 1\n",
    "\n",
    "    if (zeros_votes > ones_votes):\n",
    "        classification = 0\n",
    "    else:\n",
    "        classification = 1\n",
    "\n",
    "    return classification\n",
    "\n",
    "predictions_3 = KNN(x_test, x_train, y_train, 3)\n",
    "predictions_5 = KNN(x_test, x_train, y_train, 5)\n",
    "\n",
    "knn_accuracy_3 = np.mean(predictions_3 == y_test)\n",
    "knn_accuracy_5 = np.mean(predictions_5 == y_test)\n",
    "\n",
    "print(f'KNN accuracy with k = 3: {round(knn_accuracy_3*100, 2)}%')\n",
    "print(f'KNN accuracy with k = 5: {round(knn_accuracy_5*100, 2)}%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naïve Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_priori_prob_zeros = np.sum(y_train == 0) / M\n",
    "a_priori_prob_ones = np.sum(y_train == 1) / M\n",
    "\n",
    "estimate_means_zeros = np.mean(x_train[y_train == 0], axis = 0)\n",
    "estimate_means_ones = np.mean(x_train[y_train == 1], axis = 0)\n",
    "\n",
    "estimate_covariance_zeros = np.cov(x_train[y_train == 0], rowvar = False)\n",
    "estimate_covariance_ones = np.cov(x_train[y_train == 1], rowvar = False)\n",
    "\n",
    "#Print shape of all variables\n",
    "print(f'Shape of estimate_means_zeros = {estimate_means_zeros.shape} = (N, )')\n",
    "print(f'Shape of estimate_means_ones = {estimate_means_ones.shape} = (N, )')\n",
    "print(f'Shape of estimate_covariance_zeros = {estimate_covariance_zeros.shape} = (N, N)')\n",
    "print(f'Shape of estimate_covariance_ones = {estimate_covariance_ones.shape} = (N, N)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multivariate_normal_gaussian(X, mu, sigma):\n",
    "    prob = (1 / ( (2*np.pi)**(N/2) * np.sqrt(np.linalg.det(sigma)))) * np.exp((-1/2) * np.matmul((X - mu).T, np.matmul(np.linalg.inv(sigma), (X - mu))))\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_naive_gaussian(test_point):\n",
    "    # INPUTS:   test_point: (N, )\n",
    "    \n",
    "    # OUTPUTS:  classification: an integer indicating the classification of the test point\n",
    "    #                           either 0 (No Fraud) or 1 (Fraud)\n",
    "    \n",
    "    prob_zeros = multivariate_normal_gaussian(test_point, estimate_means_zeros, estimate_covariance_zeros)\n",
    "    prob_ones = multivariate_normal_gaussian(test_point, estimate_means_ones, estimate_covariance_ones)\n",
    "\n",
    "    if prob_zeros > prob_ones:\n",
    "        classification = 0\n",
    "    else:\n",
    "        classification = 1\n",
    "    \n",
    "    return classification\n",
    "\n",
    "naive_bayes_accuracy = 0\n",
    "for i in range(len(x_test)):\n",
    "    classification = predict_naive_gaussian(x_test[i])\n",
    "    if (classification == y_test[i]):\n",
    "        naive_bayes_accuracy += 1\n",
    "naive_bayes_accuracy /= len(x_test)\n",
    "print(f'Naive Bayes accuracy: {round(naive_bayes_accuracy*100, 2)}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
