{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numba import jit, njit\n",
    "\n",
    "CAT_COL = [\"repeat_retailer\", \"used_chip\", \"used_pin_number\", \"online_order\", \"fraud\"]\n",
    "NUM_COL = [\"distance_from_home\", \"distance_from_last_transaction\", \"ratio_to_median_purchase_price\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting(df : pd.DataFrame, col : str):\n",
    "    df.plot.scatter(x=col, y=\"fraud\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance_from_home [57.87785658 10.8299427   5.09107949 ...  2.91485699  4.25872939\n",
      " 58.10812496]\n",
      "distance_from_last_transaction [0.31114001 0.1755915  0.80515259 ... 1.47268669 0.24202337 0.31811012]\n",
      "ratio_to_median_purchase_price [1.94593998 1.29421881 0.42771456 ... 0.21807549 0.47582206 0.38691985]\n",
      "repeat_retailer [1. 0.]\n",
      "used_chip [1. 0.]\n",
      "used_pin_number [0. 1.]\n",
      "online_order [0. 1.]\n",
      "fraud [0. 1.]\n"
     ]
    }
   ],
   "source": [
    "dataSet = pd.read_csv('card_transdata.csv')\n",
    "#print levels of every column\n",
    "for col in dataSet.columns:\n",
    "    print(col, dataSet[col].unique())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes:\n",
    "\n",
    "## 1. Payment Methods and Transaction Data\n",
    "\n",
    "When analyzing transaction data, it's important to consider the different payment methods that may have been used. The `used_chip`, `used_pin_number`, and `online_order` variables can provide valuable insights into the payment methods used for each transaction.\n",
    "\n",
    "Here are some possible interpretations of different combinations of these variables:\n",
    "\n",
    "- A row with `used_chip = 0`, `used_pin_number = 0`, and `online_order = 0` could indicate that the payment was made using a wireless payment method, such as a mobile payment app that uses Bluetooth or NFC technology to make payments.\n",
    "\n",
    "- A row with `used_chip = 1`, `used_pin_number = 0`, and `online_order = 1` could indicate that the payment was made using NFC technology, which allows users to make contactless payments using their smartphones or other devices.\n",
    "\n",
    "Other combinations of these variables could also indicate the use of alternative payment methods, such as bank transfers, prepaid cards, or cryptocurrency.\n",
    "\n",
    "When cleaning and preprocessing the data, it's important to carefully consider the implications of missing or incomplete data and to handle outliers appropriately. Additionally, z-score normalization can be used to standardize the scale of variables in the dataset, and feature engineering can be used to create new variables that may improve the predictive power of the model.\n",
    "\n",
    "Overall, understanding the different payment methods used in the transaction data can help to uncover valuable insights and improve the accuracy and effectiveness of the analysis.\n",
    "\n",
    "## 2. Outliers:\n",
    "\n",
    "The `distance_from_home`, `distance_from_last_transaction`, and `ratio_to_median_purchase_price` columns may contain outliers that are valid and meaningful data points. For example, a transaction that is much farther from home or the last transaction may indicate a change in the consumer's behavior or circumstances. Similarly, a purchase that is significantly above or below the median price may reflect the consumer's preferences or availability of products.\n",
    "Therefore, removing all outliers in these columns may result in a biased or incomplete representation of your data and lead to erroneous conclusions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSet.dropna(inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>distance_from_home</th>\n",
       "      <th>distance_from_last_transaction</th>\n",
       "      <th>ratio_to_median_purchase_price</th>\n",
       "      <th>repeat_retailer</th>\n",
       "      <th>used_chip</th>\n",
       "      <th>used_pin_number</th>\n",
       "      <th>online_order</th>\n",
       "      <th>fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>24.515258</td>\n",
       "      <td>4.182657</td>\n",
       "      <td>1.752482</td>\n",
       "      <td>0.881536</td>\n",
       "      <td>0.350399</td>\n",
       "      <td>0.100608</td>\n",
       "      <td>0.650552</td>\n",
       "      <td>0.087403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>40.944157</td>\n",
       "      <td>9.552231</td>\n",
       "      <td>2.167108</td>\n",
       "      <td>0.323157</td>\n",
       "      <td>0.477095</td>\n",
       "      <td>0.300809</td>\n",
       "      <td>0.476796</td>\n",
       "      <td>0.282425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.382177</td>\n",
       "      <td>0.015167</td>\n",
       "      <td>0.077271</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.878008</td>\n",
       "      <td>0.296671</td>\n",
       "      <td>0.475673</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.967760</td>\n",
       "      <td>0.998650</td>\n",
       "      <td>0.997717</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>25.743985</td>\n",
       "      <td>3.355748</td>\n",
       "      <td>2.096370</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>259.943898</td>\n",
       "      <td>65.725606</td>\n",
       "      <td>12.794086</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       distance_from_home  distance_from_last_transaction  \\\n",
       "count      1000000.000000                  1000000.000000   \n",
       "mean            24.515258                        4.182657   \n",
       "std             40.944157                        9.552231   \n",
       "min              0.382177                        0.015167   \n",
       "25%              3.878008                        0.296671   \n",
       "50%              9.967760                        0.998650   \n",
       "75%             25.743985                        3.355748   \n",
       "max            259.943898                       65.725606   \n",
       "\n",
       "       ratio_to_median_purchase_price  repeat_retailer       used_chip  \\\n",
       "count                  1000000.000000   1000000.000000  1000000.000000   \n",
       "mean                         1.752482         0.881536        0.350399   \n",
       "std                          2.167108         0.323157        0.477095   \n",
       "min                          0.077271         0.000000        0.000000   \n",
       "25%                          0.475673         1.000000        0.000000   \n",
       "50%                          0.997717         1.000000        0.000000   \n",
       "75%                          2.096370         1.000000        1.000000   \n",
       "max                         12.794086         1.000000        1.000000   \n",
       "\n",
       "       used_pin_number    online_order           fraud  \n",
       "count   1000000.000000  1000000.000000  1000000.000000  \n",
       "mean          0.100608        0.650552        0.087403  \n",
       "std           0.300809        0.476796        0.282425  \n",
       "min           0.000000        0.000000        0.000000  \n",
       "25%           0.000000        0.000000        0.000000  \n",
       "50%           0.000000        1.000000        0.000000  \n",
       "75%           0.000000        1.000000        0.000000  \n",
       "max           1.000000        1.000000        1.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Handle outliers\n",
    "def handle_outliers(df):\n",
    "    # Select numerical columns only\n",
    "    num_cols = df.select_dtypes(include=[np.number])\n",
    "    \n",
    "    # Compute the 1st and 99th percentile of each numerical column\n",
    "    percentiles = np.nanpercentile(num_cols, [1, 99], axis=0)\n",
    "\n",
    "    # Winsorize the numerical columns\n",
    "    num_cols = np.clip(num_cols, percentiles[0], percentiles[1])\n",
    "\n",
    "    # Replace the original numerical columns in the dataframe with the winsorized ones\n",
    "    df[num_cols.columns] = num_cols\n",
    "\n",
    "handle_outliers(dataSet)\n",
    "\n",
    "# for col in NUM_COL:\n",
    "#     plotting(dataSet, col)\n",
    "\n",
    "dataSet.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "### PCA: Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(X):\n",
    "    # Center the data\n",
    "    Y = X - np.mean(X, axis=0)\n",
    "\n",
    "    # Compute the covariance matrix\n",
    "    cov = np.cov(Y.T)\n",
    "    \n",
    "    # Compute Eigenvectors matrix\n",
    "    eigenvectors, _, _ = np.linalg.svd(cov)\n",
    "\n",
    "    # Project data onto new subspace\n",
    "    Z = np.matmul(Y, eigenvectors)\n",
    "\n",
    "    return Z\n",
    "\n",
    "dataSet[NUM_COL] = pd.DataFrame(pca(np.array(dataSet[NUM_COL])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize the data\n",
    "Note how now the numerical columns all have $mean$ $\\approx$ 0 and $std$ = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>distance_from_home</th>\n",
       "      <th>distance_from_last_transaction</th>\n",
       "      <th>ratio_to_median_purchase_price</th>\n",
       "      <th>repeat_retailer</th>\n",
       "      <th>used_chip</th>\n",
       "      <th>used_pin_number</th>\n",
       "      <th>online_order</th>\n",
       "      <th>fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.000000e+06</td>\n",
       "      <td>1.000000e+06</td>\n",
       "      <td>1.000000e+06</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-2.209788e-18</td>\n",
       "      <td>1.964118e-17</td>\n",
       "      <td>-1.596945e-17</td>\n",
       "      <td>0.881536</td>\n",
       "      <td>0.350399</td>\n",
       "      <td>0.100608</td>\n",
       "      <td>0.650552</td>\n",
       "      <td>0.087403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.323157</td>\n",
       "      <td>0.477095</td>\n",
       "      <td>0.300809</td>\n",
       "      <td>0.476796</td>\n",
       "      <td>0.282425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-5.750461e+00</td>\n",
       "      <td>-6.443660e+00</td>\n",
       "      <td>-7.754947e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-3.001099e-02</td>\n",
       "      <td>8.652462e-02</td>\n",
       "      <td>-5.891716e-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.552986e-01</td>\n",
       "      <td>3.333204e-01</td>\n",
       "      <td>-3.482811e-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.040262e-01</td>\n",
       "      <td>4.068073e-01</td>\n",
       "      <td>1.586651e-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.894527e-01</td>\n",
       "      <td>4.439574e-01</td>\n",
       "      <td>5.097903e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       distance_from_home  distance_from_last_transaction  \\\n",
       "count        1.000000e+06                    1.000000e+06   \n",
       "mean        -2.209788e-18                    1.964118e-17   \n",
       "std          1.000000e+00                    1.000000e+00   \n",
       "min         -5.750461e+00                   -6.443660e+00   \n",
       "25%         -3.001099e-02                    8.652462e-02   \n",
       "50%          3.552986e-01                    3.333204e-01   \n",
       "75%          5.040262e-01                    4.068073e-01   \n",
       "max          5.894527e-01                    4.439574e-01   \n",
       "\n",
       "       ratio_to_median_purchase_price  repeat_retailer       used_chip  \\\n",
       "count                    1.000000e+06   1000000.000000  1000000.000000   \n",
       "mean                    -1.596945e-17         0.881536        0.350399   \n",
       "std                      1.000000e+00         0.323157        0.477095   \n",
       "min                     -7.754947e-01         0.000000        0.000000   \n",
       "25%                     -5.891716e-01         1.000000        0.000000   \n",
       "50%                     -3.482811e-01         1.000000        0.000000   \n",
       "75%                      1.586651e-01         1.000000        1.000000   \n",
       "max                      5.097903e+00         1.000000        1.000000   \n",
       "\n",
       "       used_pin_number    online_order           fraud  \n",
       "count   1000000.000000  1000000.000000  1000000.000000  \n",
       "mean          0.100608        0.650552        0.087403  \n",
       "std           0.300809        0.476796        0.282425  \n",
       "min           0.000000        0.000000        0.000000  \n",
       "25%           0.000000        0.000000        0.000000  \n",
       "50%           0.000000        1.000000        0.000000  \n",
       "75%           0.000000        1.000000        0.000000  \n",
       "max           1.000000        1.000000        1.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Standardization using Z-score normalization\n",
    "def standardize(df):\n",
    "    result = df.copy()\n",
    "    for feature_name in df.columns:\n",
    "        if feature_name in CAT_COL:\n",
    "            continue\n",
    "        mean_value = df[feature_name].mean()\n",
    "        std_value = df[feature_name].std()\n",
    "        result[feature_name] = (df[feature_name] - mean_value) / std_value\n",
    "    return result\n",
    "\n",
    "dataSet = standardize(dataSet)\n",
    "\n",
    "# Describe DataSet after Standardization\n",
    "dataSet.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data\n",
    "- Test Dataset: 20%\n",
    "- Train Dataset: 80% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data and include a validation dataset\n",
    "x_train, x_test, y_train, y_test = train_test_split(dataSet.drop('fraud',axis=1), dataSet['fraud'],\n",
    "                                                     test_size=0.2, random_state=42)\n",
    "# Convert the data into numpy arrays\n",
    "x_train = x_train.values\n",
    "x_test = x_test.values\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features = 7\n",
      "Number of training examples = 800000\n",
      "Number of testing examples = 200000\n",
      "Number of classes = 2\n",
      "Shape of x_train = (800000, 7) = (M, N)\n",
      "Shape of y_train = (800000,) = (M, )\n",
      "Shape of x_test = (200000, 7) = (M_test, N)\n",
      "Shape of y_test = (200000,) = (M_test, )\n"
     ]
    }
   ],
   "source": [
    "#Number of features\n",
    "N = x_train.shape[1]\n",
    "print(f'Number of features = {N}')\n",
    "\n",
    "#Number of training examples\n",
    "M = x_train.shape[0]\n",
    "print(f'Number of training examples = {M}')\n",
    "\n",
    "#Number of testing examples\n",
    "M_test = x_test.shape[0]\n",
    "print(f'Number of testing examples = {M_test}')\n",
    "\n",
    "#Number of classes\n",
    "K = len(np.unique(y_train))\n",
    "print(f'Number of classes = {K}')\n",
    "\n",
    "# Print shapes of training and testing data\n",
    "print(f'Shape of x_train = {x_train.shape} = (M, N)')\n",
    "print(f'Shape of y_train = {y_train.shape} = (M, )')\n",
    "print(f'Shape of x_test = {x_test.shape} = (M_test, N)')\n",
    "print(f'Shape of y_test = {y_test.shape} = (M_test, )')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logisitic regression score: 96.28%\n"
     ]
    }
   ],
   "source": [
    "# Implement logistic regression\n",
    "logisticRegr = LogisticRegression(max_iter=1000)\n",
    "logisticRegr.fit(x_train, y_train)\n",
    "# predictions = logisticRegr.predict(x_test)\n",
    "# print(\"Logistic regression predictions: \", predictions[:10])\n",
    "score = logisticRegr.score(x_test, y_test)\n",
    "print(f\"Logisitic regression score: {round(score*100, 2)}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training logistic regression model...\n",
      "Training complete!\n",
      "Making predictions...\n",
      "Predictions complete!\n",
      "Logistic regression score: 92.11%\n"
     ]
    }
   ],
   "source": [
    "@njit\n",
    "def sigmoidVec(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "@njit\n",
    "def logisticFit(X,Y, numIter=1000, alpha=0.0001 ):\n",
    "    ''' X is a M x N matrix of inputs\n",
    "        Y is a M x 1 vector of labels '''\n",
    "    \n",
    "    # Get the number of training examples and number of features\n",
    "    m, n = X.shape\n",
    "\n",
    "    # Initialize the weights and bias\n",
    "    weights = np.zeros((n))\n",
    "    b = 0\n",
    "\n",
    "    for i in range(numIter):\n",
    "        # Compute the activation\n",
    "        a = sigmoidVec(np.dot(X, weights) + b)\n",
    "\n",
    "        # Compute the gradient\n",
    "        dw = (1/m)*np.dot(X.T, (a - Y))\n",
    "        db = (1/m)*np.sum(a - Y)\n",
    "\n",
    "        # Update the parameters\n",
    "        weights = weights - alpha*dw\n",
    "        b = b - alpha*db\n",
    "\n",
    "    return weights, b\n",
    "\n",
    "@njit\n",
    "def logisticPredict(X, weights, b):\n",
    "    ''' X is a M x N matrix of inputs\n",
    "        weights is a N x 1 vector of weights\n",
    "        b is a scalar bias '''\n",
    "    \n",
    "    # Get the number of training examples and number of features\n",
    "    m, n = X.shape\n",
    "\n",
    "    # Initialize the output\n",
    "    Y = np.zeros((m, 1))\n",
    "\n",
    "    # Compute the activation\n",
    "    a = sigmoidVec(np.dot(X, weights) + b)\n",
    "\n",
    "    # Threshold the activation to compute the output\n",
    "    Y = np.where(a > 0.5, 1, 0)\n",
    "\n",
    "    return Y\n",
    "\n",
    "\n",
    "print(\"Training logistic regression model...\")\n",
    "weights, b = logisticFit(x_train, y_train, numIter=1000, alpha=0.01)\n",
    "print(\"Training complete!\")\n",
    "\n",
    "\n",
    "print(\"Making predictions...\")\n",
    "predictions = logisticPredict(x_test, weights, b)\n",
    "print(\"Predictions complete!\")\n",
    "\n",
    "# Compute the accuracy\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "print(f\"Logistic regression score: {round(accuracy*100, 2)}%\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 0\n"
     ]
    }
   ],
   "source": [
    "# Get user input for one transaction\n",
    "def predict_single(model ,distance_from_home, distance_from_last_transaction, \n",
    "                       ratio_to_median_purchase_price, repeat_retailer, used_chip, \n",
    "                       used_pin_number, online_order):\n",
    "    # distance_from_home = 270\n",
    "    # distance_from_last_transaction = 500\n",
    "    # ratio_to_median_purchase_price = 1.2\n",
    "    # repeat_retailer = 0\n",
    "    # used_chip = 1\n",
    "    # used_pin_number = 0\n",
    "    # online_order = 0\n",
    "\n",
    "    # row ha shape (1, N)\n",
    "    row = np.array([[ distance_from_home,\n",
    "                    distance_from_last_transaction,\n",
    "                    ratio_to_median_purchase_price,\n",
    "                    repeat_retailer,\n",
    "                    used_chip, used_pin_number, online_order]])\n",
    "    \n",
    "    prediction = int(model.predict(row))\n",
    "    return prediction\n",
    "\n",
    "print(f'Prediction: {predict_single(logisticRegr, 5, 1, 0.8, 0, 1, 1, 0)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimum Distance Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateDistance(x1, x2):\n",
    "    # Euclidean distance\n",
    "    distance = np.linalg.norm(x1-x2)\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Distance Classifier accuracy: 91.28%\n"
     ]
    }
   ],
   "source": [
    "def MinimumDistanceClassifier(test_points, training_features, labels):\n",
    "    # INPUTS:   test_points: (M_test, N)  \n",
    "    #           training_features: (M, N)\n",
    "    #           labels: (M, )\n",
    "    \n",
    "    # OUTPUTS:  classification: an integer indicating the classification of the test point\n",
    "    #                           either 0 (No Fraud) or 1 (Fraud)\n",
    "      \n",
    "    \n",
    "    zeros_mean = np.mean(training_features[labels == 0], axis = 0)\n",
    "    ones_mean = np.mean(training_features[labels == 1], axis = 0)\n",
    "\n",
    "    zeros_dist = calculateDistance(zeros_mean, test_points)\n",
    "    ones_dist = calculateDistance(ones_mean, test_points)\n",
    "\n",
    "    if zeros_dist < ones_dist:\n",
    "        classification = 0\n",
    "    else:\n",
    "        classification = 1\n",
    "    \n",
    "    return classification\n",
    "\n",
    "mdc_accuracy = 0\n",
    "\n",
    "\n",
    "predictions = MinimumDistanceClassifier(x_test, x_train, y_train)\n",
    "mdc_accuracy = np.mean(predictions == y_test)\n",
    "print(f'Minimum Distance Classifier accuracy: {round(mdc_accuracy*100, 2)}%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NaÃ¯ve Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multivariate_normal_gaussian(X, mu, sigma):\n",
    "    prob = (1 / ( (2*np.pi)**(N/2) * np.sqrt(np.linalg.det(sigma)))) * np.exp((-1/2) * np.matmul((X - mu).T, np.matmul(np.linalg.inv(sigma), (X - mu))))\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of estimate_means_zeros = (7,) = (N, )\n",
      "Shape of estimate_means_ones = (7,) = (N, )\n",
      "Shape of estimate_covariance_zeros = (7, 7) = (N, N)\n",
      "Shape of estimate_covariance_ones = (7, 7) = (N, N)\n"
     ]
    }
   ],
   "source": [
    "a_priori_prob_zeros = np.sum(y_train == 0) / M\n",
    "a_priori_prob_ones = np.sum(y_train == 1) / M\n",
    "\n",
    "estimate_means_zeros = np.mean(x_train[y_train == 0], axis = 0)\n",
    "estimate_means_ones = np.mean(x_train[y_train == 1], axis = 0)\n",
    "\n",
    "estimate_covariance_zeros = np.cov(x_train[y_train == 0], rowvar = False)\n",
    "estimate_covariance_ones = np.cov(x_train[y_train == 1], rowvar = False)\n",
    "\n",
    "#Print shape of all variables\n",
    "print(f'Shape of estimate_means_zeros = {estimate_means_zeros.shape} = (N, )')\n",
    "print(f'Shape of estimate_means_ones = {estimate_means_ones.shape} = (N, )')\n",
    "print(f'Shape of estimate_covariance_zeros = {estimate_covariance_zeros.shape} = (N, N)')\n",
    "print(f'Shape of estimate_covariance_ones = {estimate_covariance_ones.shape} = (N, N)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes accuracy: 87.41%\n"
     ]
    }
   ],
   "source": [
    "def predict_naive_gaussian(test_point):\n",
    "    # INPUTS:   test_point: (N, )\n",
    "    \n",
    "    # OUTPUTS:  classification: an integer indicating the classification of the test point\n",
    "    #                           either 0 (No Fraud) or 1 (Fraud)\n",
    "    \n",
    "    prob_zeros = multivariate_normal_gaussian(test_point, estimate_means_zeros, estimate_covariance_zeros)\n",
    "    prob_ones = multivariate_normal_gaussian(test_point, estimate_means_ones, estimate_covariance_ones)\n",
    "\n",
    "    if prob_zeros > prob_ones:\n",
    "        classification = 0\n",
    "    else:\n",
    "        classification = 1\n",
    "    \n",
    "    return classification\n",
    "\n",
    "naive_bayes_accuracy = 0\n",
    "for i in range(len(x_test)):\n",
    "    classification = predict_naive_gaussian(x_test[i])\n",
    "    if (classification == y_test[i]):\n",
    "        naive_bayes_accuracy += 1\n",
    "naive_bayes_accuracy /= len(x_test)\n",
    "print(f'Naive Bayes accuracy: {round(naive_bayes_accuracy*100, 2)}%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN using Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN without Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\University_courses\\UniCourse\\Senior_2\\Spring 2023\\Big_data\\Credit-Card-Fraud-Detection\\project.ipynb Cell 32\u001b[0m in \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/University_courses/UniCourse/Senior_2/Spring%202023/Big_data/Credit-Card-Fraud-Detection/project.ipynb#Y151sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m         classification \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/University_courses/UniCourse/Senior_2/Spring%202023/Big_data/Credit-Card-Fraud-Detection/project.ipynb#Y151sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m classification\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/University_courses/UniCourse/Senior_2/Spring%202023/Big_data/Credit-Card-Fraud-Detection/project.ipynb#Y151sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m predictions_3 \u001b[39m=\u001b[39m KNN(x_test, x_train, y_train, \u001b[39m3\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/University_courses/UniCourse/Senior_2/Spring%202023/Big_data/Credit-Card-Fraud-Detection/project.ipynb#Y151sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m predictions_5 \u001b[39m=\u001b[39m KNN(x_test, x_train, y_train, \u001b[39m5\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/University_courses/UniCourse/Senior_2/Spring%202023/Big_data/Credit-Card-Fraud-Detection/project.ipynb#Y151sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m knn_accuracy_3 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(predictions_3 \u001b[39m==\u001b[39m y_test)\n",
      "\u001b[1;32md:\\University_courses\\UniCourse\\Senior_2\\Spring 2023\\Big_data\\Credit-Card-Fraud-Detection\\project.ipynb Cell 32\u001b[0m in \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/University_courses/UniCourse/Senior_2/Spring%202023/Big_data/Credit-Card-Fraud-Detection/project.ipynb#Y151sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mKNN\u001b[39m(test_points, training_features, labels, k):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/University_courses/UniCourse/Senior_2/Spring%202023/Big_data/Credit-Card-Fraud-Detection/project.ipynb#Y151sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m# INPUTS:   test_points: (M_test, N)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/University_courses/UniCourse/Senior_2/Spring%202023/Big_data/Credit-Card-Fraud-Detection/project.ipynb#Y151sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39m#           training_features: (M, N)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/University_courses/UniCourse/Senior_2/Spring%202023/Big_data/Credit-Card-Fraud-Detection/project.ipynb#Y151sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m# OUTPUTS:  classification: an integer indicating the classification of the test point\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/University_courses/UniCourse/Senior_2/Spring%202023/Big_data/Credit-Card-Fraud-Detection/project.ipynb#Y151sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m#                           either 0 (No Fraud) or 1 (Fraud)\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/University_courses/UniCourse/Senior_2/Spring%202023/Big_data/Credit-Card-Fraud-Detection/project.ipynb#Y151sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     distances \u001b[39m=\u001b[39m [calculateDistance(test_points, p) \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m training_features]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/University_courses/UniCourse/Senior_2/Spring%202023/Big_data/Credit-Card-Fraud-Detection/project.ipynb#Y151sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m# distances = np.sqrt(((test_points[:, np.newaxis, :] - training_features) ** 2).sum(axis=2))\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/University_courses/UniCourse/Senior_2/Spring%202023/Big_data/Credit-Card-Fraud-Detection/project.ipynb#Y151sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     k_nearest \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margpartition(distances, k)[:k]\n",
      "\u001b[1;32md:\\University_courses\\UniCourse\\Senior_2\\Spring 2023\\Big_data\\Credit-Card-Fraud-Detection\\project.ipynb Cell 32\u001b[0m in \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/University_courses/UniCourse/Senior_2/Spring%202023/Big_data/Credit-Card-Fraud-Detection/project.ipynb#Y151sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mKNN\u001b[39m(test_points, training_features, labels, k):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/University_courses/UniCourse/Senior_2/Spring%202023/Big_data/Credit-Card-Fraud-Detection/project.ipynb#Y151sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m# INPUTS:   test_points: (M_test, N)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/University_courses/UniCourse/Senior_2/Spring%202023/Big_data/Credit-Card-Fraud-Detection/project.ipynb#Y151sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39m#           training_features: (M, N)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/University_courses/UniCourse/Senior_2/Spring%202023/Big_data/Credit-Card-Fraud-Detection/project.ipynb#Y151sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m# OUTPUTS:  classification: an integer indicating the classification of the test point\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/University_courses/UniCourse/Senior_2/Spring%202023/Big_data/Credit-Card-Fraud-Detection/project.ipynb#Y151sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m#                           either 0 (No Fraud) or 1 (Fraud)\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/University_courses/UniCourse/Senior_2/Spring%202023/Big_data/Credit-Card-Fraud-Detection/project.ipynb#Y151sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     distances \u001b[39m=\u001b[39m [calculateDistance(test_points, p) \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m training_features]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/University_courses/UniCourse/Senior_2/Spring%202023/Big_data/Credit-Card-Fraud-Detection/project.ipynb#Y151sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m# distances = np.sqrt(((test_points[:, np.newaxis, :] - training_features) ** 2).sum(axis=2))\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/University_courses/UniCourse/Senior_2/Spring%202023/Big_data/Credit-Card-Fraud-Detection/project.ipynb#Y151sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     k_nearest \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margpartition(distances, k)[:k]\n",
      "\u001b[1;32md:\\University_courses\\UniCourse\\Senior_2\\Spring 2023\\Big_data\\Credit-Card-Fraud-Detection\\project.ipynb Cell 32\u001b[0m in \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/University_courses/UniCourse/Senior_2/Spring%202023/Big_data/Credit-Card-Fraud-Detection/project.ipynb#Y151sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcalculateDistance\u001b[39m(x1, x2):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/University_courses/UniCourse/Senior_2/Spring%202023/Big_data/Credit-Card-Fraud-Detection/project.ipynb#Y151sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m# Euclidean distance\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/University_courses/UniCourse/Senior_2/Spring%202023/Big_data/Credit-Card-Fraud-Detection/project.ipynb#Y151sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     distance \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mlinalg\u001b[39m.\u001b[39;49mnorm(x1\u001b[39m-\u001b[39;49mx2)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/University_courses/UniCourse/Senior_2/Spring%202023/Big_data/Credit-Card-Fraud-Detection/project.ipynb#Y151sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m distance\n",
      "File \u001b[1;32m<__array_function__ internals>:177\u001b[0m, in \u001b[0;36mnorm\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def KNN(test_points, training_features, labels, k):\n",
    "    # INPUTS:   test_points: (M_test, N)\n",
    "    #           training_features: (M, N)\n",
    "    #           k: the number of nearest neighbours. \n",
    "    \n",
    "    # OUTPUTS:  classification: an integer indicating the classification of the test point\n",
    "    #                           either 0 (No Fraud) or 1 (Fraud)\n",
    "\n",
    "    distances = [calculateDistance(test_points, p) for p in training_features]\n",
    "\n",
    "    # distances = np.sqrt(((test_points[:, np.newaxis, :] - training_features) ** 2).sum(axis=2))\n",
    "    \n",
    "    k_nearest = np.argpartition(distances, k)[:k]\n",
    "    \n",
    "    zeros_votes = 0\n",
    "    ones_votes = 0\n",
    "\n",
    "    for i in k_nearest:\n",
    "        if (labels[i] == 0):\n",
    "            zeros_votes += 1\n",
    "        if (labels[i] == 1):\n",
    "            ones_votes += 1\n",
    "\n",
    "    if (zeros_votes > ones_votes):\n",
    "        classification = 0\n",
    "    else:\n",
    "        classification = 1\n",
    "\n",
    "    return classification\n",
    "\n",
    "predictions_3 = KNN(x_test, x_train, y_train, 3)\n",
    "predictions_5 = KNN(x_test, x_train, y_train, 5)\n",
    "\n",
    "knn_accuracy_3 = np.mean(predictions_3 == y_test)\n",
    "knn_accuracy_5 = np.mean(predictions_5 == y_test)\n",
    "\n",
    "print(f'KNN accuracy with k = 3: {round(knn_accuracy_3*100, 2)}%')\n",
    "print(f'KNN accuracy with k = 5: {round(knn_accuracy_5*100, 2)}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
