{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from numba import njit, vectorize\n",
    "from multiprocessing import Pool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SparkSession\n",
    "ss = SparkSession.builder.appName(\"MapReduceExample\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data to RDD\n",
    "dataRDD = ss.read.csv(\"card_transdata.csv\", header=True, inferSchema=True).rdd\n",
    "#print first rows\n",
    "print(dataRDD.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateDistance(x1, x2):\n",
    "    # Euclidean distance\n",
    "    distance = np.linalg.norm(x1-x2)\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainRDD, testRDD = dataRDD.randomSplit([0.8, 0.2], seed=42)\n",
    "#drop fraud column from testRDD\n",
    "testlabels = testRDD.map(lambda x: x[-1])\n",
    "testRDD = testRDD.map(lambda x: x[:-1])\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN MapReduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNNMapReduce(testPoints):\n",
    "    \n",
    "    testPredictions = []\n",
    "    k = 3\n",
    "      \n",
    "\n",
    "    for testPoint in tqdm(testPoints):\n",
    "        fraudDetection = trainRDD.map(lambda x: (None, (x,calculateDistance(testPoint, np.array(x[:-1])))))\n",
    "        fraudDetection = fraudDetection.takeOrdered(k, key=lambda x: x[1][1])\n",
    "        results = fraudDetection\n",
    "        countFraud = 0\n",
    "        for result in results:\n",
    "            if  result[1][0][7] == 1:\n",
    "                countFraud += 1\n",
    "        \n",
    "        if countFraud > k/2:\n",
    "            testPredictions.append(1)\n",
    "        else:\n",
    "            testPredictions.append(0)\n",
    "\n",
    "    return testPredictions\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@njit\n",
    "def calculateDistanceVectorized(x1, x2):\n",
    "    # Euclidean distance\n",
    "    distance = np.linalg.norm(x1-x2)\n",
    "    return distance\n",
    "\n",
    "@njit\n",
    "def predictKNN(testPoint : np.array, trainRDD : np.array, k):\n",
    "\n",
    "    #Calculate distance between testPoint and all train points\n",
    "    # results = calculateDistanceVectorized(testPoint, trainRDD)\n",
    "    results = np.zeros((len(trainRDD), 1 + trainRDD.shape[1]))\n",
    "    for i in range(len(trainRDD)):\n",
    "        results[i][0] = calculateDistanceVectorized(testPoint, trainRDD[i][0:7])\n",
    "        results[i][1:] = trainRDD[i]\n",
    "\n",
    "    #Sort results by distance\n",
    "    results = trainRDD[np.argsort(results[:,0])]\n",
    "    #Take k nearest neighbors\n",
    "    results = results[:k]\n",
    "\n",
    "    #Count frauds\n",
    "    countFraud = 0\n",
    "    for i in range(len(results)):\n",
    "        if  results[i][7] == 1:\n",
    "            countFraud += 1\n",
    "        \n",
    "    if countFraud > k/2:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "@njit\n",
    "def normalKNN(testPoints, trainRDD, k = 1):\n",
    "    testPredictions = np.zeros(len(testPoints))\n",
    "\n",
    "    # with Pool() as p:\n",
    "    #     testPredictions = p.map(predictKNN, testPoints)\n",
    "\n",
    "\n",
    "    for i in range(len(testPoints)):\n",
    "        testPredictions[i] = predictKNN(testPoints[i], trainRDD, k)\n",
    "    # for i in tqdm(range(len(testPoints))):\n",
    "    #     testPredictions[i] = predictKNN(testPoints[i])\n",
    "\n",
    "    return testPredictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def testKNN(testPoints, trainRDD, testLabels):\n",
    "    # testPredictions = np.array(KNNMapReduce())\n",
    "    testPredictions = normalKNN(testPoints, trainRDD)\n",
    "    # print(testPredictions)\n",
    "    testLabelsArr = np.array(testLabels)\n",
    "    # print(testLabels)\n",
    "    accuracy = np.sum(testPredictions == testLabelsArr)/len(testLabelsArr)\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "\n",
    "print(\"Collecting test points\")\n",
    "testPoints = np.array(testRDD.collect()[300:500])\n",
    "print(\"Finished collecting test points\")\n",
    "print(\"Collecting train points\")\n",
    "trainData = np.array(trainRDD.collect())\n",
    "print(\"Finished collecting train points\")\n",
    "\n",
    "\n",
    "\n",
    "testKNN(testPoints, trainData, testlabels.collect()[300:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save testPredictions and test labels to same text file\n",
    "# #open file\n",
    "# f = open(\"testPredictions2.txt\", \"w\")\n",
    "# #write testPredictions and test labels beside each other to file\n",
    "# for i in range(len(testPredictions)):\n",
    "#     f.write(str(testPredictions[i]) + \" \" + str(testLabels[i]) + \"\\n\")\n",
    "# #close file\n",
    "# f.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering (K-means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calCentroid(x):\n",
    "        centroid = np.mean(x, axis=0)\n",
    "        return centroid\n",
    "\n",
    "def initializeCentroids(dataRDD, k):\n",
    "    centroids = dataRDD.takeSample(False, k, seed=42)\n",
    "    return centroids\n",
    "\n",
    "def assignCluster(x, centroids) -> int:\n",
    "    distances = np.zeros(len(centroids))\n",
    "    for i in range(len(centroids)):\n",
    "        distances[i]= calculateDistance(x, centroids[i])\n",
    "    cluster = np.argmin(distances)\n",
    "    return cluster\n",
    "\n",
    "def KMeans(iter: int = 5):\n",
    "    \n",
    "\n",
    "    k = 2\n",
    "    centroids = initializeCentroids(dataRDD, k)\n",
    "    centroids = np.array(centroids)\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"Training KNN... {iter} iterations\")\n",
    "    for i in tqdm(range(iter)):\n",
    "        #Train KNN\n",
    "        fraudDetection = dataRDD.map(lambda x: (assignCluster(np.array(x),centroids), x))\n",
    "        fraudDetection = fraudDetection.groupByKey().map(lambda x: (x[0], list(x[1])))\n",
    "        fraudDetection = fraudDetection.reduceByKey(lambda x,y: x+y)\n",
    "        fraudDetection = fraudDetection.map(lambda x: (x[0], calCentroid(np.array(x[1]))))\n",
    "        fraudDetection = fraudDetection.collect()\n",
    "\n",
    "        # print(fraudDetection.take(11))\n",
    "\n",
    "        firstCentroid = fraudDetection[0]\n",
    "        secondCentroid = fraudDetection[1]\n",
    "\n",
    "        # print(firstCentroid)\n",
    "        # print(secondCentroid)\n",
    "\n",
    "        centroids = np.array([firstCentroid[1], secondCentroid[1]])\n",
    "    \n",
    "    return centroids\n",
    "\n",
    "def KMeansWSS(centroids, dataRDD):\n",
    "    WSS = 0\n",
    "    data = dataRDD.collect()\n",
    "    print(\"Calculating WSS...\")\n",
    "    for i in tqdm(range(len(data))):\n",
    "        tstPt = np.array(data[i])\n",
    "        cluster = assignCluster(tstPt, centroids)\n",
    "        WSS += calculateDistance(tstPt, centroids[cluster]) ** 2\n",
    "\n",
    "    return WSS\n",
    "\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "centroids = KMeans(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WSS = KMeansWSS(centroids, dataRDD)\n",
    "print(\"WSS: \", WSS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
